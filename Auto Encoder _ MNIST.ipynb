{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17caa60a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "284a3fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Conv2D, Conv2DTranspose, Dense, Flatten, Dropout, BatchNormalization, Reshape, LeakyReLU\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import tensorflow as tf\n",
    "Input = tf.keras.Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d061ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6ac5287",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000, 28, 28), (60000,))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(x_train, y_train), (x_valid, y_valid) = mnist.load_data()\n",
    "x_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "28c74e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.reshape(-1, 28, 28, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f1a7ad6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-1.0, 1.0)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = x_train / 127.5 - 1\n",
    "x_train.min(), x_train.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f5367dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/reejungkim/opt/anaconda3/envs/myenv/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "encoder_input = Input(shape=(28, 28, 1))\n",
    "\n",
    "# 28 X 28\n",
    "x = Conv2D(32, 3, padding='same')(encoder_input) \n",
    "x = BatchNormalization()(x)\n",
    "x = LeakyReLU()(x) \n",
    "\n",
    "# 28 X 28 -> 14 X 14\n",
    "x = Conv2D(64, 3, strides=2, padding='same')(x)\n",
    "x = BatchNormalization()(x) \n",
    "x = LeakyReLU()(x) \n",
    "\n",
    "# 14 X 14 -> 7 X 7\n",
    "x = Conv2D(64, 3, strides=2, padding='same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = LeakyReLU()(x)\n",
    "\n",
    "# 17 X 7\n",
    "x = Conv2D(64, 3, padding='same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = LeakyReLU()(x)\n",
    "\n",
    "x = Flatten()(x)\n",
    "\n",
    "# 2D 좌표로 표기하기 위하여 2를 출력값으로 지정합니다.\n",
    "encoder_output = Dense(2)(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "daaa4a7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 28, 28, 1)]       0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 28, 28, 32)        320       \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 28, 28, 32)        128       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)      (None, 28, 28, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 14, 14, 64)        18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 14, 14, 64)        256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 7, 7, 64)          36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 7, 7, 64)          256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 7, 7, 64)          36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 7, 7, 64)          256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 3136)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2)                 6274      \n",
      "=================================================================\n",
      "Total params: 99,842\n",
      "Trainable params: 99,394\n",
      "Non-trainable params: 448\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder = Model(encoder_input, encoder_output)\n",
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "edd3909b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input으로는 2D 좌표가 들어갑니다.\n",
    "decoder_input = Input(shape=(2, ))\n",
    "\n",
    "# 2D 좌표를 7*7*64 개의 neuron 출력 값을 가지도록 변경합니다.\n",
    "x = Dense(7*7*64)(decoder_input)\n",
    "x = Reshape( (7, 7, 64))(x)\n",
    "\n",
    "# 7 X 7 -> 7 X 7\n",
    "x = Conv2DTranspose(64, 3, strides=1, padding='same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = LeakyReLU()(x)\n",
    "\n",
    "# 7 X 7 -> 14 X 14\n",
    "x = Conv2DTranspose(64, 3, strides=2, padding='same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = LeakyReLU()(x)\n",
    "\n",
    "# 14 X 14 -> 28 X 28\n",
    "x = Conv2DTranspose(64, 3, strides=2, padding='same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = LeakyReLU()(x)\n",
    "\n",
    "# 28 X 28 -> 28 X 28\n",
    "x = Conv2DTranspose(32, 3, strides=1, padding='same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = LeakyReLU()(x)\n",
    "\n",
    "# 최종 output\n",
    "decoder_output = Conv2DTranspose(1, 3, strides=1, padding='same', activation='tanh')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "01795490",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 2)]               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 3136)              9408      \n",
      "_________________________________________________________________\n",
      "reshape (Reshape)            (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose (Conv2DTran (None, 7, 7, 64)          36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 7, 7, 64)          256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTr (None, 14, 14, 64)        36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 14, 14, 64)        256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTr (None, 28, 28, 64)        36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 28, 28, 64)        256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)    (None, 28, 28, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_3 (Conv2DTr (None, 28, 28, 32)        18464     \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 28, 28, 32)        128       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)    (None, 28, 28, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_4 (Conv2DTr (None, 28, 28, 1)         289       \n",
      "=================================================================\n",
      "Total params: 139,841\n",
      "Trainable params: 139,393\n",
      "Non-trainable params: 448\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "decoder = Model(decoder_input, decoder_output)\n",
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e920123b",
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.0005\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "encoder_in = Input(shape=(28, 28, 1))\n",
    "x = encoder(encoder_in)\n",
    "decoder_out = decoder(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7b5063d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_encoder = Model(encoder_in, decoder_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2e1edf0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = 'tmp/01-basic-auto-encoder-MNIST.ckpt'\n",
    "checkpoint = ModelCheckpoint(checkpoint_path, \n",
    "                             save_best_only=True, \n",
    "                             save_weights_only=True, \n",
    "                             monitor='loss', \n",
    "                             verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2b288347",
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_encoder.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "#history = model.fit(X_train, np.array(y_train), epochs=4, batch_size=64, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "524410b1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/reejungkim/opt/anaconda3/envs/myenv/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Epoch 1/100\n",
      "59968/60000 [============================>.] - ETA: 0s - loss: -11.4521 - acc: 3.8028e-04\n",
      "Epoch 00001: loss improved from inf to -11.45197, saving model to tmp/01-basic-auto-encoder-MNIST.ckpt\n",
      "60000/60000 [==============================] - 246s 4ms/sample - loss: -11.4520 - acc: 3.8095e-04\n",
      "Epoch 2/100\n",
      "59968/60000 [============================>.] - ETA: 0s - loss: -11.6124 - acc: 9.5461e-04\n",
      "Epoch 00002: loss improved from -11.45197 to -11.61233, saving model to tmp/01-basic-auto-encoder-MNIST.ckpt\n",
      "60000/60000 [==============================] - 240s 4ms/sample - loss: -11.6123 - acc: 9.5419e-04\n",
      "Epoch 3/100\n",
      "59968/60000 [============================>.] - ETA: 0s - loss: -11.6595 - acc: 0.0010\n",
      "Epoch 00003: loss improved from -11.61233 to -11.65940, saving model to tmp/01-basic-auto-encoder-MNIST.ckpt\n",
      "60000/60000 [==============================] - 232s 4ms/sample - loss: -11.6594 - acc: 0.0010\n",
      "Epoch 4/100\n",
      "59968/60000 [============================>.] - ETA: 0s - loss: -11.6748 - acc: 0.0011\n",
      "Epoch 00004: loss improved from -11.65940 to -11.67476, saving model to tmp/01-basic-auto-encoder-MNIST.ckpt\n",
      "60000/60000 [==============================] - 231s 4ms/sample - loss: -11.6748 - acc: 0.0011\n",
      "Epoch 5/100\n",
      "59968/60000 [============================>.] - ETA: 0s - loss: -11.6852 - acc: 0.0011\n",
      "Epoch 00005: loss improved from -11.67476 to -11.68521, saving model to tmp/01-basic-auto-encoder-MNIST.ckpt\n",
      "60000/60000 [==============================] - 232s 4ms/sample - loss: -11.6852 - acc: 0.0011\n",
      "Epoch 6/100\n",
      "59968/60000 [============================>.] - ETA: 0s - loss: -11.6937 - acc: 0.0011\n",
      "Epoch 00006: loss improved from -11.68521 to -11.69339, saving model to tmp/01-basic-auto-encoder-MNIST.ckpt\n",
      "60000/60000 [==============================] - 234s 4ms/sample - loss: -11.6934 - acc: 0.0011\n",
      "Epoch 7/100\n",
      "59968/60000 [============================>.] - ETA: 0s - loss: -11.7058 - acc: 0.0012\n",
      "Epoch 00007: loss improved from -11.69339 to -11.70585, saving model to tmp/01-basic-auto-encoder-MNIST.ckpt\n",
      "60000/60000 [==============================] - 246s 4ms/sample - loss: -11.7058 - acc: 0.0012\n",
      "Epoch 8/100\n",
      "59968/60000 [============================>.] - ETA: 0s - loss: -11.7143 - acc: 0.0011\n",
      "Epoch 00008: loss improved from -11.70585 to -11.71441, saving model to tmp/01-basic-auto-encoder-MNIST.ckpt\n",
      "60000/60000 [==============================] - 252s 4ms/sample - loss: -11.7144 - acc: 0.0012\n",
      "Epoch 9/100\n",
      "59968/60000 [============================>.] - ETA: 0s - loss: -11.7179 - acc: 0.0011\n",
      "Epoch 00009: loss improved from -11.71441 to -11.71787, saving model to tmp/01-basic-auto-encoder-MNIST.ckpt\n",
      "60000/60000 [==============================] - 240s 4ms/sample - loss: -11.7179 - acc: 0.0011\n",
      "Epoch 10/100\n",
      "59968/60000 [============================>.] - ETA: 0s - loss: -11.7276 - acc: 0.0012\n",
      "Epoch 00010: loss improved from -11.71787 to -11.72749, saving model to tmp/01-basic-auto-encoder-MNIST.ckpt\n",
      "60000/60000 [==============================] - 242s 4ms/sample - loss: -11.7275 - acc: 0.0012\n",
      "Epoch 11/100\n",
      "59968/60000 [============================>.] - ETA: 0s - loss: -11.7332 - acc: 0.0013\n",
      "Epoch 00011: loss improved from -11.72749 to -11.73323, saving model to tmp/01-basic-auto-encoder-MNIST.ckpt\n",
      "60000/60000 [==============================] - 241s 4ms/sample - loss: -11.7332 - acc: 0.0013\n",
      "Epoch 12/100\n",
      "59968/60000 [============================>.] - ETA: 0s - loss: -11.7406 - acc: 0.0012\n",
      "Epoch 00012: loss improved from -11.73323 to -11.74062, saving model to tmp/01-basic-auto-encoder-MNIST.ckpt\n",
      "60000/60000 [==============================] - 239s 4ms/sample - loss: -11.7406 - acc: 0.0012\n",
      "Epoch 13/100\n",
      "59968/60000 [============================>.] - ETA: 0s - loss: -11.7471 - acc: 0.0012\n",
      "Epoch 00013: loss improved from -11.74062 to -11.74704, saving model to tmp/01-basic-auto-encoder-MNIST.ckpt\n",
      "60000/60000 [==============================] - 248s 4ms/sample - loss: -11.7470 - acc: 0.0012\n",
      "Epoch 14/100\n",
      "59968/60000 [============================>.] - ETA: 0s - loss: -11.7522 - acc: 0.0013\n",
      "Epoch 00014: loss improved from -11.74704 to -11.75242, saving model to tmp/01-basic-auto-encoder-MNIST.ckpt\n",
      "60000/60000 [==============================] - 244s 4ms/sample - loss: -11.7524 - acc: 0.0013\n",
      "Epoch 15/100\n",
      "59968/60000 [============================>.] - ETA: 0s - loss: -11.7548 - acc: 0.0013\n",
      "Epoch 00015: loss improved from -11.75242 to -11.75468, saving model to tmp/01-basic-auto-encoder-MNIST.ckpt\n",
      "60000/60000 [==============================] - 237s 4ms/sample - loss: -11.7547 - acc: 0.0013\n",
      "Epoch 16/100\n",
      "59968/60000 [============================>.] - ETA: 0s - loss: -11.7613 - acc: 0.0012\n",
      "Epoch 00016: loss improved from -11.75468 to -11.76122, saving model to tmp/01-basic-auto-encoder-MNIST.ckpt\n",
      "60000/60000 [==============================] - 257s 4ms/sample - loss: -11.7612 - acc: 0.0013\n",
      "Epoch 17/100\n",
      "59968/60000 [============================>.] - ETA: 0s - loss: -11.7645 - acc: 0.0013\n",
      "Epoch 00017: loss improved from -11.76122 to -11.76433, saving model to tmp/01-basic-auto-encoder-MNIST.ckpt\n",
      "60000/60000 [==============================] - 269s 4ms/sample - loss: -11.7643 - acc: 0.0013\n",
      "Epoch 18/100\n",
      "59968/60000 [============================>.] - ETA: 0s - loss: -11.7667 - acc: 0.0013\n",
      "Epoch 00018: loss improved from -11.76433 to -11.76669, saving model to tmp/01-basic-auto-encoder-MNIST.ckpt\n",
      "60000/60000 [==============================] - 317s 5ms/sample - loss: -11.7667 - acc: 0.0013\n",
      "Epoch 19/100\n",
      "59968/60000 [============================>.] - ETA: 0s - loss: -11.7697 - acc: 0.0013\n",
      "Epoch 00019: loss improved from -11.76669 to -11.76973, saving model to tmp/01-basic-auto-encoder-MNIST.ckpt\n",
      "60000/60000 [==============================] - 282s 5ms/sample - loss: -11.7697 - acc: 0.0013\n",
      "Epoch 20/100\n",
      "59968/60000 [============================>.] - ETA: 0s - loss: -11.7767 - acc: 0.0013\n",
      "Epoch 00020: loss improved from -11.76973 to -11.77656, saving model to tmp/01-basic-auto-encoder-MNIST.ckpt\n",
      "60000/60000 [==============================] - 276s 5ms/sample - loss: -11.7766 - acc: 0.0013\n",
      "Epoch 21/100\n",
      "59968/60000 [============================>.] - ETA: 0s - loss: -11.7833 - acc: 0.0013\n",
      "Epoch 00021: loss improved from -11.77656 to -11.78327, saving model to tmp/01-basic-auto-encoder-MNIST.ckpt\n",
      "60000/60000 [==============================] - 270s 5ms/sample - loss: -11.7833 - acc: 0.0013\n",
      "Epoch 22/100\n",
      "59968/60000 [============================>.] - ETA: 0s - loss: -11.7865 - acc: 0.0013\n",
      "Epoch 00022: loss improved from -11.78327 to -11.78646, saving model to tmp/01-basic-auto-encoder-MNIST.ckpt\n",
      "60000/60000 [==============================] - 271s 5ms/sample - loss: -11.7865 - acc: 0.0013\n",
      "Epoch 23/100\n",
      "59968/60000 [============================>.] - ETA: 0s - loss: -11.7889 - acc: 0.0014\n",
      "Epoch 00023: loss improved from -11.78646 to -11.78873, saving model to tmp/01-basic-auto-encoder-MNIST.ckpt\n",
      "60000/60000 [==============================] - 275s 5ms/sample - loss: -11.7887 - acc: 0.0014\n",
      "Epoch 24/100\n",
      "59968/60000 [============================>.] - ETA: 0s - loss: -11.7952 - acc: 0.0014\n",
      "Epoch 00024: loss improved from -11.78873 to -11.79520, saving model to tmp/01-basic-auto-encoder-MNIST.ckpt\n",
      "60000/60000 [==============================] - 280s 5ms/sample - loss: -11.7952 - acc: 0.0014\n",
      "Epoch 25/100\n",
      "59968/60000 [============================>.] - ETA: 0s - loss: -11.7933 - acc: 0.0013\n",
      "Epoch 00025: loss did not improve from -11.79520\n",
      "60000/60000 [==============================] - 285s 5ms/sample - loss: -11.7935 - acc: 0.0013\n",
      "Epoch 26/100\n",
      "59968/60000 [============================>.] - ETA: 0s - loss: -11.7942 - acc: 0.0013\n",
      "Epoch 00026: loss did not improve from -11.79520\n",
      "60000/60000 [==============================] - 291s 5ms/sample - loss: -11.7941 - acc: 0.0013\n",
      "Epoch 27/100\n",
      "59968/60000 [============================>.] - ETA: 0s - loss: -11.7967 - acc: 0.0013\n",
      "Epoch 00027: loss improved from -11.79520 to -11.79684, saving model to tmp/01-basic-auto-encoder-MNIST.ckpt\n",
      "60000/60000 [==============================] - 297s 5ms/sample - loss: -11.7968 - acc: 0.0013\n",
      "Epoch 28/100\n",
      "59968/60000 [============================>.] - ETA: 0s - loss: -11.8026 - acc: 0.0014\n",
      "Epoch 00028: loss improved from -11.79684 to -11.80269, saving model to tmp/01-basic-auto-encoder-MNIST.ckpt\n",
      "60000/60000 [==============================] - 294s 5ms/sample - loss: -11.8027 - acc: 0.0014\n",
      "Epoch 29/100\n",
      "59968/60000 [============================>.] - ETA: 0s - loss: -11.8076 - acc: 0.0013\n",
      "Epoch 00029: loss improved from -11.80269 to -11.80761, saving model to tmp/01-basic-auto-encoder-MNIST.ckpt\n",
      "60000/60000 [==============================] - 296s 5ms/sample - loss: -11.8076 - acc: 0.0013\n",
      "Epoch 30/100\n",
      "59968/60000 [============================>.] - ETA: 0s - loss: -11.8056 - acc: 0.0014\n",
      "Epoch 00030: loss did not improve from -11.80761\n",
      "60000/60000 [==============================] - 294s 5ms/sample - loss: -11.8057 - acc: 0.0014\n",
      "Epoch 31/100\n",
      "59968/60000 [============================>.] - ETA: 0s - loss: -11.8138 - acc: 0.0015\n",
      "Epoch 00031: loss improved from -11.80761 to -11.81386, saving model to tmp/01-basic-auto-encoder-MNIST.ckpt\n",
      "60000/60000 [==============================] - 295s 5ms/sample - loss: -11.8139 - acc: 0.0015\n",
      "Epoch 32/100\n",
      "59968/60000 [============================>.] - ETA: 0s - loss: -11.8163 - acc: 0.0014\n",
      "Epoch 00032: loss improved from -11.81386 to -11.81646, saving model to tmp/01-basic-auto-encoder-MNIST.ckpt\n",
      "60000/60000 [==============================] - 270s 4ms/sample - loss: -11.8165 - acc: 0.0014\n",
      "Epoch 33/100\n",
      "59968/60000 [============================>.] - ETA: 0s - loss: -11.8157 - acc: 0.0014\n",
      "Epoch 00033: loss did not improve from -11.81646\n",
      "60000/60000 [==============================] - 236s 4ms/sample - loss: -11.8156 - acc: 0.0014\n",
      "Epoch 34/100\n",
      "59968/60000 [============================>.] - ETA: 0s - loss: -11.8198 - acc: 0.0015\n",
      "Epoch 00034: loss improved from -11.81646 to -11.82002, saving model to tmp/01-basic-auto-encoder-MNIST.ckpt\n",
      "60000/60000 [==============================] - 233s 4ms/sample - loss: -11.8200 - acc: 0.0015\n",
      "Epoch 35/100\n",
      "59968/60000 [============================>.] - ETA: 0s - loss: -11.8221 - acc: 0.0014\n",
      "Epoch 00035: loss improved from -11.82002 to -11.82198, saving model to tmp/01-basic-auto-encoder-MNIST.ckpt\n",
      "60000/60000 [==============================] - 236s 4ms/sample - loss: -11.8220 - acc: 0.0014\n",
      "Epoch 36/100\n",
      "59968/60000 [============================>.] - ETA: 0s - loss: -11.8248 - acc: 0.0015\n",
      "Epoch 00036: loss improved from -11.82198 to -11.82494, saving model to tmp/01-basic-auto-encoder-MNIST.ckpt\n",
      "60000/60000 [==============================] - 237s 4ms/sample - loss: -11.8249 - acc: 0.0015\n",
      "Epoch 37/100\n",
      "59968/60000 [============================>.] - ETA: 0s - loss: -11.8262 - acc: 0.0015\n",
      "Epoch 00037: loss improved from -11.82494 to -11.82609, saving model to tmp/01-basic-auto-encoder-MNIST.ckpt\n",
      "60000/60000 [==============================] - 237s 4ms/sample - loss: -11.8261 - acc: 0.0015\n",
      "Epoch 38/100\n",
      "59968/60000 [============================>.] - ETA: 0s - loss: -11.8271 - acc: 0.0015\n",
      "Epoch 00038: loss improved from -11.82609 to -11.82713, saving model to tmp/01-basic-auto-encoder-MNIST.ckpt\n",
      "60000/60000 [==============================] - 235s 4ms/sample - loss: -11.8271 - acc: 0.0015\n",
      "Epoch 39/100\n",
      "59968/60000 [============================>.] - ETA: 0s - loss: -11.8247 - acc: 0.0015\n",
      "Epoch 00039: loss did not improve from -11.82713\n",
      "60000/60000 [==============================] - 239s 4ms/sample - loss: -11.8246 - acc: 0.0015\n",
      "Epoch 40/100\n",
      "59968/60000 [============================>.] - ETA: 0s - loss: -11.8287 - acc: 0.0015\n",
      "Epoch 00040: loss improved from -11.82713 to -11.82886, saving model to tmp/01-basic-auto-encoder-MNIST.ckpt\n",
      "60000/60000 [==============================] - 238s 4ms/sample - loss: -11.8289 - acc: 0.0015\n",
      "Epoch 41/100\n",
      "59968/60000 [============================>.] - ETA: 0s - loss: -11.8316 - acc: 0.0016\n",
      "Epoch 00041: loss improved from -11.82886 to -11.83168, saving model to tmp/01-basic-auto-encoder-MNIST.ckpt\n",
      "60000/60000 [==============================] - 238s 4ms/sample - loss: -11.8317 - acc: 0.0016\n",
      "Epoch 42/100\n",
      "59968/60000 [============================>.] - ETA: 0s - loss: -11.8372 - acc: 0.0015\n",
      "Epoch 00042: loss improved from -11.83168 to -11.83704, saving model to tmp/01-basic-auto-encoder-MNIST.ckpt\n",
      "60000/60000 [==============================] - 237s 4ms/sample - loss: -11.8370 - acc: 0.0015\n",
      "Epoch 43/100\n",
      "59968/60000 [============================>.] - ETA: 0s - loss: -11.8332 - acc: 0.0015\n",
      "Epoch 00043: loss did not improve from -11.83704\n",
      "60000/60000 [==============================] - 237s 4ms/sample - loss: -11.8331 - acc: 0.0015\n",
      "Epoch 44/100\n",
      "59968/60000 [============================>.] - ETA: 0s - loss: -11.8352 - acc: 0.0015\n",
      "Epoch 00044: loss did not improve from -11.83704\n",
      "60000/60000 [==============================] - 237s 4ms/sample - loss: -11.8352 - acc: 0.0015\n",
      "Epoch 45/100\n",
      "59968/60000 [============================>.] - ETA: 0s - loss: -11.8396 - acc: 0.0016\n",
      "Epoch 00045: loss improved from -11.83704 to -11.83938, saving model to tmp/01-basic-auto-encoder-MNIST.ckpt\n",
      "60000/60000 [==============================] - 236s 4ms/sample - loss: -11.8394 - acc: 0.0016\n",
      "Epoch 46/100\n",
      "59968/60000 [============================>.] - ETA: 0s - loss: -11.8446 - acc: 0.0015\n",
      "Epoch 00046: loss improved from -11.83938 to -11.84463, saving model to tmp/01-basic-auto-encoder-MNIST.ckpt\n",
      "60000/60000 [==============================] - 236s 4ms/sample - loss: -11.8446 - acc: 0.0015\n",
      "Epoch 47/100\n",
      "59968/60000 [============================>.] - ETA: 0s - loss: -11.8458 - acc: 0.0015\n",
      "Epoch 00047: loss improved from -11.84463 to -11.84597, saving model to tmp/01-basic-auto-encoder-MNIST.ckpt\n",
      "60000/60000 [==============================] - 235s 4ms/sample - loss: -11.8460 - acc: 0.0015\n",
      "Epoch 48/100\n",
      "59968/60000 [============================>.] - ETA: 0s - loss: -11.8426 - acc: 0.0015\n",
      "Epoch 00048: loss did not improve from -11.84597\n",
      "60000/60000 [==============================] - 234s 4ms/sample - loss: -11.8427 - acc: 0.0015\n",
      "Epoch 49/100\n",
      "59968/60000 [============================>.] - ETA: 0s - loss: -11.8488 - acc: 0.0015\n",
      "Epoch 00049: loss improved from -11.84597 to -11.84877, saving model to tmp/01-basic-auto-encoder-MNIST.ckpt\n",
      "60000/60000 [==============================] - 234s 4ms/sample - loss: -11.8488 - acc: 0.0015\n",
      "Epoch 50/100\n",
      "59968/60000 [============================>.] - ETA: 0s - loss: -11.8486 - acc: 0.0015\n",
      "Epoch 00050: loss did not improve from -11.84877\n",
      "60000/60000 [==============================] - 235s 4ms/sample - loss: -11.8485 - acc: 0.0015\n",
      "Epoch 51/100\n",
      "59968/60000 [============================>.] - ETA: 0s - loss: -11.8445 - acc: 0.0016\n",
      "Epoch 00051: loss did not improve from -11.84877\n",
      "60000/60000 [==============================] - 234s 4ms/sample - loss: -11.8446 - acc: 0.0016\n",
      "Epoch 52/100\n",
      "59968/60000 [============================>.] - ETA: 0s - loss: -11.8462 - acc: 0.0016\n",
      "Epoch 00052: loss did not improve from -11.84877\n",
      "60000/60000 [==============================] - 234s 4ms/sample - loss: -11.8462 - acc: 0.0016\n",
      "Epoch 53/100\n",
      "59968/60000 [============================>.] - ETA: 0s - loss: -11.8515 - acc: 0.0016\n",
      "Epoch 00053: loss improved from -11.84877 to -11.85166, saving model to tmp/01-basic-auto-encoder-MNIST.ckpt\n",
      "60000/60000 [==============================] - 235s 4ms/sample - loss: -11.8517 - acc: 0.0016\n",
      "Epoch 54/100\n",
      "59968/60000 [============================>.] - ETA: 0s - loss: -11.8524 - acc: 0.0016\n",
      "Epoch 00054: loss improved from -11.85166 to -11.85213, saving model to tmp/01-basic-auto-encoder-MNIST.ckpt\n",
      "60000/60000 [==============================] - 234s 4ms/sample - loss: -11.8521 - acc: 0.0016\n",
      "Epoch 55/100\n",
      "59968/60000 [============================>.] - ETA: 0s - loss: -11.8534 - acc: 0.0016\n",
      "Epoch 00055: loss improved from -11.85213 to -11.85348, saving model to tmp/01-basic-auto-encoder-MNIST.ckpt\n",
      "60000/60000 [==============================] - 235s 4ms/sample - loss: -11.8535 - acc: 0.0016\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/100\n",
      "59968/60000 [============================>.] - ETA: 0s - loss: -11.8551 - acc: 0.0015\n",
      "Epoch 00056: loss improved from -11.85348 to -11.85523, saving model to tmp/01-basic-auto-encoder-MNIST.ckpt\n",
      "60000/60000 [==============================] - 235s 4ms/sample - loss: -11.8552 - acc: 0.0015\n",
      "Epoch 57/100\n",
      "59968/60000 [============================>.] - ETA: 0s - loss: -11.8595 - acc: 0.0015\n",
      "Epoch 00057: loss improved from -11.85523 to -11.85948, saving model to tmp/01-basic-auto-encoder-MNIST.ckpt\n",
      "60000/60000 [==============================] - 234s 4ms/sample - loss: -11.8595 - acc: 0.0015\n",
      "Epoch 58/100\n",
      "59968/60000 [============================>.] - ETA: 0s - loss: -11.8610 - acc: 0.0017\n",
      "Epoch 00058: loss improved from -11.85948 to -11.86102, saving model to tmp/01-basic-auto-encoder-MNIST.ckpt\n",
      "60000/60000 [==============================] - 235s 4ms/sample - loss: -11.8610 - acc: 0.0017\n",
      "Epoch 59/100\n",
      "59968/60000 [============================>.] - ETA: 0s - loss: -11.8620 - acc: 0.0016\n",
      "Epoch 00059: loss improved from -11.86102 to -11.86188, saving model to tmp/01-basic-auto-encoder-MNIST.ckpt\n",
      "60000/60000 [==============================] - 235s 4ms/sample - loss: -11.8619 - acc: 0.0016\n",
      "Epoch 60/100\n",
      "59968/60000 [============================>.] - ETA: 0s - loss: -11.8617 - acc: 0.0015\n",
      "Epoch 00060: loss did not improve from -11.86188\n",
      "60000/60000 [==============================] - 235s 4ms/sample - loss: -11.8616 - acc: 0.0015\n",
      "Epoch 61/100\n",
      "59968/60000 [============================>.] - ETA: 0s - loss: -11.8619 - acc: 0.0016\n",
      "Epoch 00061: loss did not improve from -11.86188\n",
      "60000/60000 [==============================] - 235s 4ms/sample - loss: -11.8619 - acc: 0.0016\n",
      "Epoch 62/100\n",
      "59968/60000 [============================>.] - ETA: 0s - loss: -11.8689 - acc: 0.0017\n",
      "Epoch 00062: loss improved from -11.86188 to -11.86916, saving model to tmp/01-basic-auto-encoder-MNIST.ckpt\n",
      "60000/60000 [==============================] - 234s 4ms/sample - loss: -11.8692 - acc: 0.0017\n",
      "Epoch 63/100\n",
      "59968/60000 [============================>.] - ETA: 0s - loss: -11.8705 - acc: 0.0016\n",
      "Epoch 00063: loss improved from -11.86916 to -11.87041, saving model to tmp/01-basic-auto-encoder-MNIST.ckpt\n",
      "60000/60000 [==============================] - 235s 4ms/sample - loss: -11.8704 - acc: 0.0016\n",
      "Epoch 64/100\n",
      "59968/60000 [============================>.] - ETA: 0s - loss: -11.8665 - acc: 0.0016\n",
      "Epoch 00064: loss did not improve from -11.87041\n",
      "60000/60000 [==============================] - 235s 4ms/sample - loss: -11.8666 - acc: 0.0016\n",
      "Epoch 65/100\n",
      "59968/60000 [============================>.] - ETA: 0s - loss: -11.8708 - acc: 0.0016\n",
      "Epoch 00065: loss improved from -11.87041 to -11.87077, saving model to tmp/01-basic-auto-encoder-MNIST.ckpt\n",
      "60000/60000 [==============================] - 235s 4ms/sample - loss: -11.8708 - acc: 0.0016\n",
      "Epoch 66/100\n",
      "59968/60000 [============================>.] - ETA: 0s - loss: -11.8704 - acc: 0.0017\n",
      "Epoch 00066: loss did not improve from -11.87077\n",
      "60000/60000 [==============================] - 235s 4ms/sample - loss: -11.8703 - acc: 0.0017\n",
      "Epoch 67/100\n",
      "59968/60000 [============================>.] - ETA: 0s - loss: -11.8704 - acc: 0.0016\n",
      "Epoch 00067: loss did not improve from -11.87077\n",
      "60000/60000 [==============================] - 235s 4ms/sample - loss: -11.8706 - acc: 0.0016\n",
      "Epoch 68/100\n",
      "59968/60000 [============================>.] - ETA: 0s - loss: -11.8711 - acc: 0.0015\n",
      "Epoch 00068: loss improved from -11.87077 to -11.87110, saving model to tmp/01-basic-auto-encoder-MNIST.ckpt\n",
      "60000/60000 [==============================] - 235s 4ms/sample - loss: -11.8711 - acc: 0.0015\n",
      "Epoch 69/100\n",
      "59968/60000 [============================>.] - ETA: 0s - loss: -11.8712 - acc: 0.0016\n",
      "Epoch 00069: loss improved from -11.87110 to -11.87129, saving model to tmp/01-basic-auto-encoder-MNIST.ckpt\n",
      "60000/60000 [==============================] - 236s 4ms/sample - loss: -11.8713 - acc: 0.0016\n",
      "Epoch 70/100\n",
      "59968/60000 [============================>.] - ETA: 0s - loss: -11.8726 - acc: 0.0016\n",
      "Epoch 00070: loss improved from -11.87129 to -11.87276, saving model to tmp/01-basic-auto-encoder-MNIST.ckpt\n",
      "60000/60000 [==============================] - 238s 4ms/sample - loss: -11.8728 - acc: 0.0016\n",
      "Epoch 71/100\n",
      "59968/60000 [============================>.] - ETA: 0s - loss: -11.8784 - acc: 0.0016\n",
      "Epoch 00071: loss improved from -11.87276 to -11.87824, saving model to tmp/01-basic-auto-encoder-MNIST.ckpt\n",
      "60000/60000 [==============================] - 238s 4ms/sample - loss: -11.8782 - acc: 0.0016\n",
      "Epoch 72/100\n",
      "59968/60000 [============================>.] - ETA: 0s - loss: -11.8765 - acc: 0.0017\n",
      "Epoch 00072: loss did not improve from -11.87824\n",
      "60000/60000 [==============================] - 237s 4ms/sample - loss: -11.8765 - acc: 0.0017\n",
      "Epoch 73/100\n",
      "59968/60000 [============================>.] - ETA: 0s - loss: -11.8812 - acc: 0.0016\n",
      "Epoch 00073: loss improved from -11.87824 to -11.88131, saving model to tmp/01-basic-auto-encoder-MNIST.ckpt\n",
      "60000/60000 [==============================] - 236s 4ms/sample - loss: -11.8813 - acc: 0.0016\n",
      "Epoch 74/100\n",
      "59968/60000 [============================>.] - ETA: 0s - loss: -11.8789 - acc: 0.0016\n",
      "Epoch 00074: loss did not improve from -11.88131\n",
      "60000/60000 [==============================] - 236s 4ms/sample - loss: -11.8791 - acc: 0.0016\n",
      "Epoch 75/100\n",
      "59968/60000 [============================>.] - ETA: 0s - loss: -11.8818 - acc: 0.0016\n",
      "Epoch 00075: loss improved from -11.88131 to -11.88181, saving model to tmp/01-basic-auto-encoder-MNIST.ckpt\n",
      "60000/60000 [==============================] - 236s 4ms/sample - loss: -11.8818 - acc: 0.0016\n",
      "Epoch 76/100\n",
      "59968/60000 [============================>.] - ETA: 0s - loss: -11.8828 - acc: 0.0017\n",
      "Epoch 00076: loss improved from -11.88181 to -11.88288, saving model to tmp/01-basic-auto-encoder-MNIST.ckpt\n",
      "60000/60000 [==============================] - 236s 4ms/sample - loss: -11.8829 - acc: 0.0017\n",
      "Epoch 77/100\n",
      "59968/60000 [============================>.] - ETA: 0s - loss: -11.8769 - acc: 0.0016\n",
      "Epoch 00077: loss did not improve from -11.88288\n",
      "60000/60000 [==============================] - 234s 4ms/sample - loss: -11.8770 - acc: 0.0016\n",
      "Epoch 78/100\n",
      "59968/60000 [============================>.] - ETA: 0s - loss: -11.8789 - acc: 0.0017\n",
      "Epoch 00078: loss did not improve from -11.88288\n",
      "60000/60000 [==============================] - 234s 4ms/sample - loss: -11.8789 - acc: 0.0017\n",
      "Epoch 79/100\n",
      "59968/60000 [============================>.] - ETA: 0s - loss: -11.8853 - acc: 0.0016\n",
      "Epoch 00079: loss improved from -11.88288 to -11.88535, saving model to tmp/01-basic-auto-encoder-MNIST.ckpt\n",
      "60000/60000 [==============================] - 234s 4ms/sample - loss: -11.8854 - acc: 0.0016\n",
      "Epoch 80/100\n",
      "59968/60000 [============================>.] - ETA: 0s - loss: -11.8823 - acc: 0.0017\n",
      "Epoch 00080: loss did not improve from -11.88535\n",
      "60000/60000 [==============================] - 234s 4ms/sample - loss: -11.8825 - acc: 0.0017\n",
      "Epoch 81/100\n",
      "59968/60000 [============================>.] - ETA: 0s - loss: -11.8881 - acc: 0.0017\n",
      "Epoch 00081: loss improved from -11.88535 to -11.88820, saving model to tmp/01-basic-auto-encoder-MNIST.ckpt\n",
      "60000/60000 [==============================] - 234s 4ms/sample - loss: -11.8882 - acc: 0.0017\n",
      "Epoch 82/100\n",
      "59968/60000 [============================>.] - ETA: 0s - loss: -11.8832 - acc: 0.0016\n",
      "Epoch 00082: loss did not improve from -11.88820\n",
      "60000/60000 [==============================] - 234s 4ms/sample - loss: -11.8833 - acc: 0.0016\n",
      "Epoch 83/100\n",
      "59968/60000 [============================>.] - ETA: 0s - loss: -11.8882 - acc: 0.0017\n",
      "Epoch 00083: loss improved from -11.88820 to -11.88835, saving model to tmp/01-basic-auto-encoder-MNIST.ckpt\n",
      "60000/60000 [==============================] - 234s 4ms/sample - loss: -11.8884 - acc: 0.0017\n",
      "Epoch 84/100\n",
      "59968/60000 [============================>.] - ETA: 0s - loss: -11.8887 - acc: 0.0016\n",
      "Epoch 00084: loss improved from -11.88835 to -11.88866, saving model to tmp/01-basic-auto-encoder-MNIST.ckpt\n",
      "60000/60000 [==============================] - 234s 4ms/sample - loss: -11.8887 - acc: 0.0016\n",
      "Epoch 85/100\n",
      "59968/60000 [============================>.] - ETA: 0s - loss: -11.8886 - acc: 0.0018\n",
      "Epoch 00085: loss did not improve from -11.88866\n",
      "60000/60000 [==============================] - 235s 4ms/sample - loss: -11.8885 - acc: 0.0018\n",
      "Epoch 86/100\n",
      "59968/60000 [============================>.] - ETA: 0s - loss: -11.8918 - acc: 0.0017\n",
      "Epoch 00086: loss improved from -11.88866 to -11.89182, saving model to tmp/01-basic-auto-encoder-MNIST.ckpt\n",
      "60000/60000 [==============================] - 234s 4ms/sample - loss: -11.8918 - acc: 0.0017\n",
      "Epoch 87/100\n",
      "59968/60000 [============================>.] - ETA: 0s - loss: -11.8905 - acc: 0.0017\n",
      "Epoch 00087: loss did not improve from -11.89182\n",
      "60000/60000 [==============================] - 235s 4ms/sample - loss: -11.8905 - acc: 0.0017\n",
      "Epoch 88/100\n",
      "59968/60000 [============================>.] - ETA: 0s - loss: -11.8899 - acc: 0.0017\n",
      "Epoch 00088: loss did not improve from -11.89182\n",
      "60000/60000 [==============================] - 235s 4ms/sample - loss: -11.8899 - acc: 0.0017\n",
      "Epoch 89/100\n",
      "59968/60000 [============================>.] - ETA: 0s - loss: -11.8907 - acc: 0.0017\n",
      "Epoch 00089: loss did not improve from -11.89182\n",
      "60000/60000 [==============================] - 235s 4ms/sample - loss: -11.8907 - acc: 0.0017\n",
      "Epoch 90/100\n",
      "59968/60000 [============================>.] - ETA: 0s - loss: -11.8915 - acc: 0.0017\n",
      "Epoch 00090: loss did not improve from -11.89182\n",
      "60000/60000 [==============================] - 235s 4ms/sample - loss: -11.8916 - acc: 0.0017\n",
      "Epoch 91/100\n",
      "59968/60000 [============================>.] - ETA: 0s - loss: -11.8925 - acc: 0.0017\n",
      "Epoch 00091: loss improved from -11.89182 to -11.89247, saving model to tmp/01-basic-auto-encoder-MNIST.ckpt\n",
      "60000/60000 [==============================] - 235s 4ms/sample - loss: -11.8925 - acc: 0.0017\n",
      "Epoch 92/100\n",
      "59968/60000 [============================>.] - ETA: 0s - loss: -11.8967 - acc: 0.0017\n",
      "Epoch 00092: loss improved from -11.89247 to -11.89677, saving model to tmp/01-basic-auto-encoder-MNIST.ckpt\n",
      "60000/60000 [==============================] - 235s 4ms/sample - loss: -11.8968 - acc: 0.0017\n",
      "Epoch 93/100\n",
      "59968/60000 [============================>.] - ETA: 0s - loss: -11.8934 - acc: 0.0017\n",
      "Epoch 00093: loss did not improve from -11.89677\n",
      "60000/60000 [==============================] - 235s 4ms/sample - loss: -11.8934 - acc: 0.0017\n",
      "Epoch 94/100\n",
      "59968/60000 [============================>.] - ETA: 0s - loss: -11.8958 - acc: 0.0017\n",
      "Epoch 00094: loss did not improve from -11.89677\n",
      "60000/60000 [==============================] - 235s 4ms/sample - loss: -11.8960 - acc: 0.0017\n",
      "Epoch 95/100\n",
      "59968/60000 [============================>.] - ETA: 0s - loss: -11.8946 - acc: 0.0017\n",
      "Epoch 00095: loss did not improve from -11.89677\n",
      "60000/60000 [==============================] - 235s 4ms/sample - loss: -11.8947 - acc: 0.0017\n",
      "Epoch 96/100\n",
      "59968/60000 [============================>.] - ETA: 0s - loss: -11.8938 - acc: 0.0017\n",
      "Epoch 00096: loss did not improve from -11.89677\n",
      "60000/60000 [==============================] - 235s 4ms/sample - loss: -11.8939 - acc: 0.0017\n",
      "Epoch 97/100\n",
      "59968/60000 [============================>.] - ETA: 0s - loss: -11.8994 - acc: 0.0017\n",
      "Epoch 00097: loss improved from -11.89677 to -11.89945, saving model to tmp/01-basic-auto-encoder-MNIST.ckpt\n",
      "60000/60000 [==============================] - 235s 4ms/sample - loss: -11.8994 - acc: 0.0017\n",
      "Epoch 98/100\n",
      "59968/60000 [============================>.] - ETA: 0s - loss: -11.8984 - acc: 0.0017\n",
      "Epoch 00098: loss did not improve from -11.89945\n",
      "60000/60000 [==============================] - 235s 4ms/sample - loss: -11.8985 - acc: 0.0017\n",
      "Epoch 99/100\n",
      "59968/60000 [============================>.] - ETA: 0s - loss: -11.9021 - acc: 0.0017\n",
      "Epoch 00099: loss improved from -11.89945 to -11.90210, saving model to tmp/01-basic-auto-encoder-MNIST.ckpt\n",
      "60000/60000 [==============================] - 235s 4ms/sample - loss: -11.9021 - acc: 0.0017\n",
      "Epoch 100/100\n",
      "59968/60000 [============================>.] - ETA: 0s - loss: -11.8977 - acc: 0.0017\n",
      "Epoch 00100: loss did not improve from -11.90210\n",
      "60000/60000 [==============================] - 235s 4ms/sample - loss: -11.8977 - acc: 0.0017\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7ffcd5d0fef0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auto_encoder.fit(x_train, x_train, \n",
    "                 batch_size=BATCH_SIZE, \n",
    "                 epochs=100, \n",
    "                 callbacks=[checkpoint], \n",
    "                )\n",
    "auto_encoder.load_weights(checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb61100f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
